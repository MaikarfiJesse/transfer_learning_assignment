{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaikarfiJesse/transfer_learning_assignment/blob/main/Transfer_model_Dates_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyYx40NfyFAP"
      },
      "source": [
        "# Dates fruits Classification\n",
        "\n",
        "This project is aimed to use pretrained models with a new dataset to classify different types of dates fruits from their images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0uMj4jYyTn9"
      },
      "source": [
        "##Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K8vqt7mt05Fs"
      },
      "outputs": [],
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.initializers import GlorotNormal\n",
        "from tensorflow.keras.metrics import TruePositives, FalsePositives, TrueNegatives, FalseNegatives, BinaryAccuracy, Precision, Recall, AUC\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import cv2\n",
        "import pathlib\n",
        "import matplotlib as mpl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMZqt94Jy6k9"
      },
      "source": [
        "##INSTALL DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_369igJYB9fi"
      },
      "outputs": [],
      "source": [
        "img_width = 448\n",
        "img_height = 448\n",
        "batch_size = 64\n",
        "color = 3\n",
        "dataset_folder = \"date-fruit-image-dataset-in-controlled-environment\"\n",
        "dataset_url = 'https://www.kaggle.com/wadhasnalhamdan/date-fruit-image-dataset-in-controlled-environment'\n",
        "classes_name = ['Ajwa', 'Galaxy', 'Medjool', 'Meneifi', 'Nabtat Ali', 'Rutab', 'Shaishe', 'Sokari', 'Sugaey']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNyft1hvCCny",
        "outputId": "222e10d3-6666-48ac-ead6-b48ba903357a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.66.4)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.6.14)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.7)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.31.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.7)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: jessemaikarfi\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/wadhasnalhamdan/date-fruit-image-dataset-in-controlled-environment\n",
            "Downloading date-fruit-image-dataset-in-controlled-environment.zip to ./date-fruit-image-dataset-in-controlled-environment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3.11G/3.11G [00:45<00:00, 74.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets\n",
        "\n",
        "import opendatasets as od\n",
        "od.download(dataset_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlvV9YjR2oqd"
      },
      "source": [
        "## Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "794zNCY4CSfg"
      },
      "outputs": [],
      "source": [
        "dataset = pathlib.Path(dataset_folder)\n",
        "\n",
        "def directory_dataset(dataset):\n",
        "  folders = []\n",
        "  for i in dataset.iterdir():\n",
        "    if i.is_dir():\n",
        "      folders.append(i)\n",
        "  return folders\n",
        "\n",
        "folders = directory_dataset(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai6Xu6cD2yOC"
      },
      "source": [
        "### Total number of Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYZvwILSB9iP",
        "outputId": "7b0b43c8-5468-437c-f748-96b52b6d66f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of all images in dataset: 1658\n"
          ]
        }
      ],
      "source": [
        "def number_of_images_in_dataset(dataset):\n",
        "  images = list(dataset.glob(\"*/*.*\"))\n",
        "  return len(images)\n",
        "print(\"number of all images in dataset: {}\".format(number_of_images_in_dataset(dataset)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0S2rZr72t09"
      },
      "source": [
        "###number of Images per folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qp0gsWl8CiCK",
        "outputId": "4382e59a-da8a-4534-c6b8-d42fdb0032be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date-fruit-image-dataset-in-controlled-environment/Galaxy: 190\n",
            "date-fruit-image-dataset-in-controlled-environment/Rutab: 146\n",
            "date-fruit-image-dataset-in-controlled-environment/Meneifi: 232\n",
            "date-fruit-image-dataset-in-controlled-environment/Nabtat Ali: 177\n",
            "date-fruit-image-dataset-in-controlled-environment/Medjool: 135\n",
            "date-fruit-image-dataset-in-controlled-environment/Shaishe: 171\n",
            "date-fruit-image-dataset-in-controlled-environment/Sokari: 264\n",
            "date-fruit-image-dataset-in-controlled-environment/Sugaey: 168\n",
            "date-fruit-image-dataset-in-controlled-environment/Ajwa: 175\n"
          ]
        }
      ],
      "source": [
        "def number_of_images_in_each_folder(folders):\n",
        "  for i in folders:\n",
        "    str_ = \"{}: {}\".format(i, len(list(pathlib.Path(i).glob(\"*.*\"))))\n",
        "    print(str_)\n",
        "\n",
        "number_of_images_in_each_folder(folders)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c_Ru24C3Hmz"
      },
      "source": [
        "###Blurring images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yu7KSoAgCiE-"
      },
      "outputs": [],
      "source": [
        "def Blurring(image):\n",
        "  image = cv2.medianBlur(image,5)\n",
        "  return image\n",
        "\n",
        "def image_generator(dataset_folder):\n",
        "  datagen = ImageDataGenerator( rescale=1/255, validation_split=0.1,vertical_flip=True ,horizontal_flip=True,width_shift_range=0.2,height_shift_range=0.2,\n",
        "                               rotation_range = 5, shear_range = 0.02,zoom_range = 0.02, preprocessing_function = Blurring)\n",
        "\n",
        "  train_generator = datagen.flow_from_directory(\n",
        "      dataset_folder,\n",
        "      target_size=(img_height, img_width),\n",
        "      batch_size=batch_size,\n",
        "      class_mode='categorical',\n",
        "      shuffle=True,\n",
        "      subset='training')\n",
        "\n",
        "  validation_generator = datagen.flow_from_directory(\n",
        "      dataset_folder,\n",
        "      target_size=(img_height, img_width),\n",
        "      batch_size= batch_size,\n",
        "      shuffle=False,\n",
        "      class_mode='categorical',\n",
        "      subset='validation')\n",
        "\n",
        "  return [train_generator, validation_generator]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qk--6CBqC6c-",
        "outputId": "2e6606ae-6584-40ee-fb80-0ce58307742b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1496 images belonging to 9 classes.\n",
            "Found 162 images belonging to 9 classes.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "[training_dataset, validation_dataset] = image_generator(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38k6G8Fu389K"
      },
      "source": [
        "##Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1P9-sSp6I55u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16, MobileNetV2, VGG19\n",
        "from tensorflow.keras.models import Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAlybpnG4FL0"
      },
      "source": [
        "###Load pre-trained models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w498nNWKC6gF",
        "outputId": "5c07cee6-7ae9-4930-c785-41c0bb5931b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load pre-trained models\n",
        "base_models = [VGG16, MobileNetV2, VGG19]\n",
        "pre_models = []\n",
        "\n",
        "for base_model in base_models:\n",
        "    base = base_model(weights='imagenet', include_top=False, input_shape=(img_height, img_width, color))\n",
        "    pre_models.append(base)\n",
        "\n",
        "\n",
        "for pretrained_model in pre_models:\n",
        "    for layer in pretrained_model.layers:\n",
        "        layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay8BhXD74w2-"
      },
      "source": [
        "###layers modification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "agR8v63rDZcI"
      },
      "outputs": [],
      "source": [
        "models = []\n",
        "for pretrained_model in pre_models:\n",
        "    last_layer = Flatten()(pretrained_model.output)\n",
        "    final_layer = Dense(9, activation='softmax')(last_layer)\n",
        "\n",
        "    model = Model(inputs=pretrained_model.input, outputs=final_layer)\n",
        "    models.append(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xJ5zN-24sZB"
      },
      "source": [
        "###Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "k9ouGFRtEjy3"
      },
      "outputs": [],
      "source": [
        "me = [\n",
        "      TruePositives(name='tp'),\n",
        "      FalsePositives(name='fp'),\n",
        "      TrueNegatives(name='tn'),\n",
        "      FalseNegatives(name='fn'),\n",
        "      BinaryAccuracy(name='accuracy'),\n",
        "      Precision(name='precision'),\n",
        "      Recall(name='recall'),\n",
        "      AUC(name='auc')\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JluE0jg_42KQ"
      },
      "source": [
        "###Compiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "AQXjf67CDZoU"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "  model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=me)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "XyEgeYzyK0r5"
      },
      "outputs": [],
      "source": [
        "call_1 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=30, restore_best_weights=True)\n",
        "callbacks = [call_1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5dbbF57489M"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIFbkC1oEx5e",
        "outputId": "cd445c87-6cf2-4e67-eb7c-a0009b688c64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "24/24 [==============================] - 3153s 130s/step - loss: 6.7167 - tp: 233.0000 - fp: 875.0000 - tn: 11093.0000 - fn: 1263.0000 - accuracy: 0.8412 - precision: 0.2103 - recall: 0.1557 - auc: 0.5887\n",
            "Epoch 2/5\n",
            "24/24 [==============================] - 3107s 129s/step - loss: 1.8654 - tp: 518.0000 - fp: 430.0000 - tn: 11538.0000 - fn: 978.0000 - accuracy: 0.8954 - precision: 0.5464 - recall: 0.3463 - auc: 0.8199\n",
            "Epoch 3/5\n",
            "24/24 [==============================] - 3297s 137s/step - loss: 0.9083 - tp: 826.0000 - fp: 190.0000 - tn: 11778.0000 - fn: 670.0000 - accuracy: 0.9361 - precision: 0.8130 - recall: 0.5521 - auc: 0.9481\n",
            "Epoch 4/5\n",
            "24/24 [==============================] - 3290s 136s/step - loss: 0.6678 - tp: 999.0000 - fp: 142.0000 - tn: 11826.0000 - fn: 497.0000 - accuracy: 0.9525 - precision: 0.8755 - recall: 0.6678 - auc: 0.9741\n",
            "Epoch 5/5\n",
            "24/24 [==============================] - 3301s 137s/step - loss: 0.7931 - tp: 1032.0000 - fp: 235.0000 - tn: 11733.0000 - fn: 464.0000 - accuracy: 0.9481 - precision: 0.8145 - recall: 0.6898 - auc: 0.9569\n"
          ]
        }
      ],
      "source": [
        "epochs=5\n",
        "history = models[0].fit(training_dataset, epochs=epochs, callbacks = callbacks, batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLJZIwIjE_2o",
        "outputId": "be41ba62-b341-4c6a-ada6-4b4487c19df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "24/24 [==============================] - 454s 18s/step - loss: 13.3019 - tp: 1736.0000 - fp: 968.0000 - tn: 22968.0000 - fn: 1256.0000 - accuracy: 0.9174 - precision: 0.6420 - recall: 0.5802 - auc: 0.8253\n",
            "Epoch 2/3\n",
            "24/24 [==============================] - 442s 18s/step - loss: 3.2388 - tp: 1131.0000 - fp: 361.0000 - tn: 11607.0000 - fn: 365.0000 - accuracy: 0.9461 - precision: 0.7580 - recall: 0.7560 - auc: 0.9002\n",
            "Epoch 3/3\n",
            "24/24 [==============================] - 425s 17s/step - loss: 2.2845 - tp: 1253.0000 - fp: 243.0000 - tn: 11725.0000 - fn: 243.0000 - accuracy: 0.9639 - precision: 0.8376 - recall: 0.8376 - auc: 0.9304\n"
          ]
        }
      ],
      "source": [
        "epochs=3\n",
        "history = models[1].fit(training_dataset, epochs=epochs, callbacks = callbacks, batch_size = batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNoJNteQLyM9",
        "outputId": "b2931683-2da0-44e7-d7af-21e7315e4528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            " 1/24 [>.............................] - ETA: 1:26:31 - loss: 1.4309 - tp: 30.0000 - fp: 23.0000 - tn: 489.0000 - fn: 34.0000 - accuracy: 0.9010 - precision: 0.5660 - recall: 0.4688 - auc: 0.8888"
          ]
        }
      ],
      "source": [
        "epochs=3\n",
        "history = models[2].fit(training_dataset, epochs=epochs, callbacks = callbacks, batch_size = batch_size\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}